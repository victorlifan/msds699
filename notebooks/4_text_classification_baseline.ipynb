{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a272a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b148b7",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "In this notebook we will explore a basic text classification pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c3cbb",
   "metadata": {},
   "source": [
    "## Subjectivity Dataset\n",
    "The subjectivity dataset has 5000 subjective and 5000 objective processed sentences. To get the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87d5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_dataset():\n",
    "    ! wget http://www.cs.cornell.edu/people/pabo/movie-review-data/rotten_imdb.tar.gz\n",
    "    ! mkdir data\n",
    "    ! tar -xvf rotten_imdb.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2ecd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpack_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4732108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plot.tok.gt9.5000   quote.tok.gt9.5000  subjdata.README.1.0\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af14e093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \r\n",
      "emerging from the human psyche and showing characteristics of abstract expressionism , minimalism and russian constructivism , graffiti removal has secured its place in the history of modern art while being created by artists who are unconscious of their artistic achievements . \r\n"
     ]
    }
   ],
   "source": [
    "! head -2 data/plot.tok.gt9.5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719ef3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('data/plot.tok.gt9.5000'),\n",
       " PosixPath('data/subjdata.README.1.0'),\n",
       " PosixPath('data/quote.tok.gt9.5000')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = Path(\"data\")\n",
    "list(PATH.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5aa4d1",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "Tokenization is the task of chopping up text into pieces, called tokens. spaCy is an open-source software library for advanced Natural Language Processing. Here we will use it for tokenization. To install spaCy:\n",
    "\n",
    "pip install -U pip setuptools wheel <br>\n",
    "pip install -U spacy <br>\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b0adcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(path):\n",
    "    \"\"\" Read file returns a list of lines.\n",
    "    \"\"\"\n",
    "    with open(path, encoding = \"ISO-8859-1\") as f:\n",
    "        content = f.readlines()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cca07b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the movie begins in the past where a young boy named sam attempts to save celebi from a hunter . \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_lines = read_file(PATH/\"plot.tok.gt9.5000\")\n",
    "obj_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6dc80dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'movie', 'begins', 'in', 'the', 'past', 'where', 'a',\n",
       "       'young', 'boy', 'named', 'sam', 'attempts', 'to', 'save', 'celebi',\n",
       "       'from', 'a', 'hunter', '.'], dtype='<U8')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple tokenization by splitting on spaces\n",
    "np.array(obj_lines[0].strip().lower().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c7327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd2b50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d214614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the DET\n",
      "movie NOUN\n",
      "begins VERB\n",
      "in ADP\n",
      "the DET\n",
      "past NOUN\n",
      "where ADV\n",
      "a DET\n",
      "young ADJ\n",
      "boy NOUN\n",
      "named VERB\n",
      "sam PROPN\n",
      "attempts NOUN\n",
      "to PART\n",
      "save VERB\n",
      "celebi NOUN\n",
      "from ADP\n",
      "a DET\n",
      "hunter NOUN\n",
      ". PUNCT\n",
      "\n",
      " SPACE\n"
     ]
    }
   ],
   "source": [
    "# here is an example of what you can do with spacy.\n",
    "# I can print the token and the \"part of speach\"\n",
    "line = nlp(obj_lines[0])\n",
    "for token in line:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b803d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([the, movie, begins, in, the, past, where, a, young, boy, named,\n",
       "       sam, attempts, to, save, celebi, from, a, hunter, ., \n",
       "], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([token for token in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a745471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(s):\n",
    "    s = nlp(str(s.strip()))\n",
    "    return np.array([token for token in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5009953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([the, movie, begins, in, the, past, where, a, young, boy, named,\n",
       "       sam, attempts, to, save, celebi, from, a, hunter, .], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentence(obj_lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e3ed56",
   "metadata": {},
   "source": [
    "### Split dataset in train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d18f55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e45b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_content = read_file(PATH/\"quote.tok.gt9.5000\")\n",
    "obj_content = read_file(PATH/\"plot.tok.gt9.5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77ac8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating labels\n",
    "sub_y = np.zeros(len(sub_content))\n",
    "obj_y = np.ones(len(obj_content))\n",
    "X = np.append(sub_content, obj_content)\n",
    "y = np.append(sub_y, obj_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "837f5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04cf516e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['will god let her fall or give her a new path ? \\n',\n",
       "        \"the director's twitchy sketchbook style and adroit perspective shifts grow wearisome amid leaden pacing and indifferent craftsmanship ( most notably wretched sound design ) . \\n\",\n",
       "        \"welles groupie/scholar peter bogdanovich took a long time to do it , but he's finally provided his own broadside at publishing giant william randolph hearst . \\n\"],\n",
       "       dtype='<U693'),\n",
       " array([1., 0., 0.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3], y_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5958e90",
   "metadata": {},
   "source": [
    "### Tokenize all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14c0bdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = [tokenize_sentence(X_train[i]) for i in range(X_train.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af8aabc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_t = [tokenize_sentence(X_val[i]) for i in range(X_val.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bba30e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([will, god, let, her, fall, or, give, her, a, new, path, ?],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae313533",
   "metadata": {},
   "source": [
    "## word embeddings from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5be939f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5799826 , -1.009465  ,  0.62448525, -0.3746994 ,  0.04847589,\n",
       "        0.23544501,  0.16930383, -0.5892657 , -0.4099527 , -0.0397221 ,\n",
       "        0.09905334, -0.17823452,  1.8112568 , -0.41191506, -0.1911665 ,\n",
       "        0.26377833, -0.20858057,  0.24163836, -0.8667144 , -0.40949103,\n",
       "        0.70684826,  0.40622658,  0.00363696,  0.16584054, -0.22824581,\n",
       "        0.9838457 ,  0.71979254, -0.33249682, -0.73977196,  0.1915259 ,\n",
       "       -0.9682615 , -0.09848729,  0.06264922, -0.24534428, -0.30293864,\n",
       "       -0.5626753 ,  1.1686754 , -0.0945009 , -0.46837738, -0.47827306,\n",
       "        0.02991231,  0.34009436,  0.88246363,  0.27727336, -0.21391977,\n",
       "       -0.5353824 ,  0.5017903 , -0.09786895, -1.1179507 ,  0.5685893 ,\n",
       "        0.50877166,  0.2935202 , -0.12042533,  0.46025705,  0.3654778 ,\n",
       "        0.15245938,  0.48111284,  0.0421572 ,  1.1205167 , -0.87217855,\n",
       "        0.46665782, -0.60350204,  1.783681  ,  0.27612534,  0.00711006,\n",
       "        0.55750287, -0.8106657 , -1.1621315 , -0.46883765,  0.23266684,\n",
       "        0.28915328,  0.17552537, -0.52067924, -0.40874356, -0.26159707,\n",
       "       -0.33522364, -0.6772767 , -0.32395914,  1.1998901 , -1.2188435 ,\n",
       "       -0.65014505,  0.64367193,  0.09065545,  0.033902  , -0.71641093,\n",
       "        0.3635984 ,  0.7277261 , -0.6553073 ,  0.13236105,  0.35810634,\n",
       "       -0.0244676 ,  0.7452942 , -0.57572335,  0.2651622 , -0.13938573,\n",
       "       -0.07359036], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"am\").vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0e5c67",
   "metadata": {},
   "source": [
    "### Sentence enconding\n",
    "Each sentence is going to be represented with the average of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b3e76e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([will, god, let, her, fall, or, give, her, a, new, path, ?],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c40d67e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 96)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([x.vector for x in X_train_t[0]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aea6b170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74590b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_encoding(s):\n",
    "    v = np.array([x.vector for x in s])\n",
    "    return v.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b22d2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([sentence_encoding(X_train_t[i]) for i in range(len(X_train_t))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b9aeddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.array([sentence_encoding(X_val_t[i]) for i in range(len(X_val_t))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1930749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 96), (2000, 96))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9f858",
   "metadata": {},
   "source": [
    "### Logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b36d4a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "067f8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, C=1).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e113470e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b028d0",
   "metadata": {},
   "source": [
    "## Encoding V2: tfidf\n",
    "In the tfidf formula each word in a document is weighted by \n",
    "$$tf_{i,j} \\cdot log \\frac{N}{df_i}$$\n",
    "\n",
    "$tf_{i,j}$ is the number of occurences of word $i$ in doc $j$ <br>\n",
    "$df_i$ is the number of documents containing $i$ <br>\n",
    "$N$ is the number of documents <br>\n",
    "\n",
    "We will be encoding a sentence using a word embeddings weighted by tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bb9f73c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{will, god, let, her, fall, or, give, her, a, new, path, ?}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(X_train_t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e3425cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_per_doc(docs):\n",
    "    counts = {}\n",
    "    for i in range(len(docs)):\n",
    "        words = set([str(x) for x in docs[i]]) \n",
    "        for word in words:\n",
    "            counts[word] = counts.get(word, 0) + 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23fc58be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(X_train_t)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe3a2d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts_per_doc(X_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f672d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(counts, N):\n",
    "    tfidf = {\"UNK\": np.log(N) }\n",
    "    for w in counts:\n",
    "        tfidf[w] = np.log(N/counts[w])\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe5cbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = compute_tfidf(counts, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c8afd94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4083f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_encoding_v2(s, tfidf=tfidf):\n",
    "    v = np.array([x.vector*(tfidf.get(str(x), tfidf[\"UNK\"])) for x in s])\n",
    "    return v.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ceff2440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will 286\n",
      "god 34\n",
      "let 22\n",
      "her 664\n",
      "fall 29\n",
      "or 305\n",
      "give 46\n",
      "her 664\n",
      "a 4311\n",
      "new 303\n",
      "path 23\n",
      "? 145\n"
     ]
    }
   ],
   "source": [
    "for w in X_train_t[0]:\n",
    "    print(w, counts[str(w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61e34d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([sentence_encoding_v2(X_train_t[i]) for i in range(len(X_train_t))])\n",
    "x_val = np.array([sentence_encoding_v2(X_val_t[i]) for i in range(len(X_val_t))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "50922ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0, C=1).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "07322a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8731e92",
   "metadata": {},
   "source": [
    "## References\n",
    "https://www.cs.cornell.edu/home/llee/papers/sentiment.home.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6bfc0",
   "metadata": {},
   "source": [
    "# Lab\n",
    "Here are some ideas on how to modify this pipeline:\n",
    "1. Make a model that just uses the part of speach tags. Do you get any signal? Then add this infor to the current model.\n",
    "2. Make a version of the first model without stopwords and punctuations.\n",
    "3. Keep just top words. \n",
    "4. Use unique words per sentence.\n",
    "5. Use just adjectives and adverbes.\n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "dc41e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8474a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3418b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
